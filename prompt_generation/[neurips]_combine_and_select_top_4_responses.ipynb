{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn7qz1mGDRuE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file names and model names\n",
        "files = [\n",
        "    \"claude_bias_extracted_singles.csv\",\n",
        "    \"gpt-4.1_bias_extracted_singles.csv\",\n",
        "    \"gemini_bias_extracted_singles.csv\",\n",
        "    \"llama_bias_extracted_singles.csv\"\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "df_list = []\n",
        "\n",
        "# Iterate through each file and model name\n",
        "for file in files:\n",
        "    # Extract model name from the file name\n",
        "    model_name = file.split('_')[0]\n",
        "\n",
        "    # Read the CSV file into a dataframe\n",
        "    df = pd.read_csv(f\"{file}\")\n",
        "\n",
        "    # Add a new column for the model name\n",
        "    df['Model'] = model_name\n",
        "\n",
        "    # Append the dataframe to the list\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all dataframes into a single dataframe\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Sort the dataframe by 'Prompt_ID' and 'Attack Strategy'\n",
        "sorted_df = combined_df.sort_values(by=['Prompt_ID'])\n",
        "\n",
        "# Display the sorted dataframe\n",
        "sorted_df.to_csv('combined_bias_extracted_singles.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('combined_violent_extracted_singles.csv')\n",
        "print(len(df))\n",
        "\n",
        "invalid_prompts = [\n",
        "    \"prompt:\",\n",
        "    \"justification:\",\n",
        "    \"summary:\",\n",
        "    \"\",\n",
        "    \"summary of process:\",\n",
        "    \"style:\",\n",
        "    \"seed prompt:\",\n",
        "    \"'prompt:\",\n",
        "    \"'justification:\",\n",
        "    \"prompt 1\",\n",
        "    \"prompt 2\",\n",
        "    \"prompt 3\",\n",
        "    \"prompt 4\",\n",
        "    \"prompt 5\",\n",
        "    \"prompt 1:\",\n",
        "    \"prompt 2:\",\n",
        "    \"prompt 3:\",\n",
        "    \"prompt 4:\",\n",
        "    \"prompt 5:\",\n",
        "    \"seed prompt review\",\n",
        "    \"understanding style of attack\",\n",
        "    \"rewriting\",\n",
        "    \"creativity/justification\",\n",
        "    \"1. prompt\",\n",
        "    \"2. prompt\",\n",
        "    \"3. prompt\",\n",
        "    \"4. prompt\",\n",
        "    \"5. prompt\",\n",
        "    \"**\",\n",
        "    \"intended connotation:\",\n",
        "    \"1. prompt:\",\n",
        "    \"2. prompt:\",\n",
        "    \"3. prompt:\",\n",
        "    \"4. prompt:\",\n",
        "    \"5. prompt:\",\n",
        "    \"seed prompt analysis\",\n",
        "    \"style of attack\",\n",
        "    \"connotation\",\n",
        "    \"style of writing\",\n",
        "    \"style of writing:\",\n",
        "    \"summary of reasoning\",\n",
        "    \"prompts:\",\n",
        "    \"s:**\",\n",
        "    \"step 1:\",\n",
        "    \"step 2:\",\n",
        "    \"step 3:\",\n",
        "    \"step 4:\",\n",
        "    \"step 5:\",\n",
        "    \"connotation target:\",\n",
        "    \"method:\",\n",
        "    \"style of attack:\",\n",
        "    \"phrasal elements:\",\n",
        "    \"context:\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"step-by-step approach\",\n",
        "    \"summary of approach:\",\n",
        "    \"connotation:\",\n",
        "    \"'prompt'\",\n",
        "    \"'justification'\",\n",
        "    \"intent:\",\n",
        "    \"prompts & justifications\",\n",
        "    \"demographic indicators\",\n",
        "    \"geography\",\n",
        "    \"geography:\",\n",
        "    \"prompt':\",\n",
        "    \"geographic indicators:\",\n",
        "    \"demographic indicators:\",\n",
        "    \"approach:\",\n",
        "    \"seed prompt\",\n",
        "    \"goal:\",\n",
        "    \"reasoning\",\n",
        "    \"summary of style\",\n",
        "    \"general approach:\",\n",
        "    \"summary of style usage:\",\n",
        "    \"process used\",\n",
        "    'step 1 (style):',\n",
        "    'step 2 (style):',\n",
        "    'step 3 (style):',\n",
        "    'step 4 (style):',\n",
        "    'step 5 (style):'\n",
        "]\n",
        "\n",
        "# Normalize Prompt column (lowercase and strip whitespace)\n",
        "df['Prompt_normalized'] = df['Prompt'].astype(str).str.lower().str.strip()\n",
        "\n",
        "# Filter out the rows where the normalized prompt matches any invalid value\n",
        "df_cleaned = df[\n",
        "    (~df['Prompt_normalized'].isin(invalid_prompts)) &\n",
        "    (df['Prompt_normalized'].str.split().str.len() >= 2) &\n",
        "   (df['Prompt_normalized'].str.len() > 2)\n",
        "].copy()\n",
        "\n",
        "# Optionally drop the temporary normalized column\n",
        "df_cleaned.drop(columns=['Prompt_normalized'], inplace=True)\n",
        "\n",
        "# View the cleaned DataFrame\n",
        "print(len(df_cleaned))\n",
        "df_cleaned.to_csv('combined_violent_extracted_singles_cleaned.csv', index=False)\n",
        "\n",
        "collapsed_df = (\n",
        "    df_cleaned.groupby(['Prompt_ID'])['Prompt']\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        "    .rename(columns={'Prompt': 'Prompts'})\n",
        ")\n",
        "\n",
        "\n",
        "# Write the collapsed DataFrame to a JSON file\n",
        "collapsed_df.to_json('collapsed_violent_prompts.json', orient='records')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPdDDKA5FWgQ",
        "outputId": "8fb41630-4825-466f-a675-c4038abfcbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4289\n",
            "3774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "failures = ['bias', 'hate', 'sexual', 'violent']\n",
        "\n",
        "for failure in failures:\n",
        "    with open('collapsed_' + failure + '_prompts.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "    new_data = []\n",
        "\n",
        "    # Iterate through the list of prompt groups\n",
        "    for group in data:\n",
        "        temp = {}\n",
        "        prompt_id = group['Prompt_ID']\n",
        "        # attack_strategy = group['Attack_Strategy']\n",
        "        prompts = group['Prompts']  # List of strings\n",
        "        prompts = [prompt for prompt in prompts if len(prompt) > 15]\n",
        "\n",
        "        embeddings = model.encode(prompts)\n",
        "        num_clusters = 4\n",
        "\n",
        "        if len(embeddings) < num_clusters:\n",
        "            new_data.append({\"prompt_id\": prompt_id, \"selected_prompts\": prompts})\n",
        "            continue\n",
        "\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(embeddings)\n",
        "        labels = kmeans.labels_\n",
        "        selected_prompts = []\n",
        "\n",
        "        for cluster_id in range(num_clusters):\n",
        "            cluster_indices = np.where(labels == cluster_id)[0]\n",
        "            centroid = kmeans.cluster_centers_[cluster_id]\n",
        "            # Find the closest sentence to the centroid\n",
        "            distances = np.linalg.norm(embeddings[cluster_indices] - centroid, axis=1)\n",
        "            closest_index = cluster_indices[np.argmin(distances)]\n",
        "            selected_prompts.append(prompts[closest_index])\n",
        "\n",
        "        # print(selected_prompts)\n",
        "        new_data.append({\"prompt_id\": prompt_id, \"selected_prompts\": selected_prompts})\n",
        "\n",
        "    with open('image_ready_' + failure + '_prompts.json', 'w') as f:\n",
        "        json.dump(new_data, f, indent=4)"
      ],
      "metadata": {
        "id": "QhvExB5ZOnrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBvD_7RDQ8ec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}