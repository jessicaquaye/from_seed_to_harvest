{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Bp-WBaLD-KbP",
        "6H9VxlvlHQ3h",
        "Btn7VJLSK5yz",
        "lvJYa47sNu6y",
        "Zu27ADOwmrZu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "IpBjXQ95iqEE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define classifier-specific logic\n",
        "def check_sd_nsfw(classification):\n",
        "    return classification is True\n",
        "\n",
        "def check_q16(classification):\n",
        "    return classification == 1\n",
        "\n",
        "def check_nudenet(classification):\n",
        "    return len(classification) > 0\n",
        "\n",
        "# Dispatch map\n",
        "logic_map = {\n",
        "    \"sd_nsfw\": check_sd_nsfw,\n",
        "    \"q16\": check_q16,\n",
        "    \"nudenet\": check_nudenet,\n",
        "}\n",
        "\n",
        "def analyze_classifications(fnames, classifier):\n",
        "    is_unsafe = logic_map[classifier]\n",
        "\n",
        "    total = 0\n",
        "    unsafe = 0\n",
        "\n",
        "    for fname in fnames:\n",
        "        total = 0\n",
        "        unsafe = 0\n",
        "        with open(fname, 'r') as f:\n",
        "            fname_to_class = json.load(f)\n",
        "\n",
        "        for img, classification in fname_to_class.items():\n",
        "            if is_unsafe(classification):\n",
        "                unsafe += 1\n",
        "            total += 1\n",
        "\n",
        "        print(f\"% Flagged as unsafe by '{classifier}': {unsafe/total * 100}\")\n",
        "\n",
        "print(\"For Hybrid\")\n",
        "nudenet_fnames = ['nudenet_bias.json', 'nudenet_hate.json', 'nudenet_sexual.json','nudenet_violent.json']\n",
        "analyze_classifications(nudenet_fnames, 'nudenet')\n",
        "\n",
        "q16_fnames = ['q16_bias.json', 'q16_hate.json', 'q16_sexual.json','q16_violent.json']\n",
        "analyze_classifications(q16_fnames, 'q16')\n",
        "\n",
        "sd_fnames = ['sd_nsfw_bias.json', 'sd_nsfw_hate.json', 'sd_nsfw_sexual.json','sd_nsfw_violent.json']\n",
        "analyze_classifications(sd_fnames, 'sd_nsfw')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzFEfoANTCEg",
        "outputId": "83039d1a-d0cc-4ea4-8166-1fce968e8571"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Hybrid\n",
            "% Flagged as unsafe by 'nudenet': 31.079672375279227\n",
            "% Flagged as unsafe by 'nudenet': 31.188155097852377\n",
            "% Flagged as unsafe by 'nudenet': 35.58475997348204\n",
            "% Flagged as unsafe by 'nudenet': 25.51268682655544\n",
            "% Flagged as unsafe by 'q16': 6.406552494415488\n",
            "% Flagged as unsafe by 'q16': 13.519753719856336\n",
            "% Flagged as unsafe by 'q16': 6.216121358655383\n",
            "% Flagged as unsafe by 'q16': 20.48264561298972\n",
            "% Flagged as unsafe by 'sd_nsfw': 40.655249441548776\n",
            "% Flagged as unsafe by 'sd_nsfw': 27.413325514916075\n",
            "% Flagged as unsafe by 'sd_nsfw': 39.585851889404516\n",
            "% Flagged as unsafe by 'sd_nsfw': 37.588758130989625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define classifier-specific logic\n",
        "def check_sd_nsfw(classification):\n",
        "    return classification is True\n",
        "\n",
        "def check_q16(classification):\n",
        "    return classification == 1\n",
        "\n",
        "def check_nudenet(classification):\n",
        "    return len(classification) > 0\n",
        "\n",
        "# Dispatch map\n",
        "logic_map = {\n",
        "    \"sd_nsfw\": check_sd_nsfw,\n",
        "    \"q16\": check_q16,\n",
        "    \"nudenet\": check_nudenet,\n",
        "}\n",
        "\n",
        "def analyze_classifications(fnames, classifier):\n",
        "    is_unsafe = logic_map[classifier]\n",
        "\n",
        "    total = 0\n",
        "    unsafe = 0\n",
        "\n",
        "    for fname in fnames:\n",
        "        with open(fname, 'r') as f:\n",
        "            fname_to_class = json.load(f)\n",
        "\n",
        "        for img, classification in fname_to_class.items():\n",
        "            if is_unsafe(classification):\n",
        "                unsafe += 1\n",
        "            total += 1\n",
        "\n",
        "        print(f\"% Flagged as unsafe by '{classifier}': {unsafe/total * 100}\")\n",
        "\n",
        "print(\"For AO\")\n",
        "nudenet_fnames = ['/content/attacks_only/nudenet_AO.json']\n",
        "analyze_classifications(nudenet_fnames, 'nudenet')\n",
        "\n",
        "q16_fnames = ['/content/attacks_only/q16_AO.json']\n",
        "analyze_classifications(q16_fnames, 'q16')\n",
        "\n",
        "sd_fnames = ['/content/attacks_only/sd_nsfw_AO.json']\n",
        "analyze_classifications(sd_fnames, 'sd_nsfw')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK1fxtddwvWy",
        "outputId": "3dff488d-8d82-4426-9ca5-9570d4df6322"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For AO\n",
            "% Flagged as unsafe by 'nudenet': 51.369426751592364\n",
            "% Flagged as unsafe by 'q16': 12.48407643312102\n",
            "% Flagged as unsafe by 'sd_nsfw': 1.5286624203821657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define classifier-specific logic\n",
        "def check_sd_nsfw(classification):\n",
        "    return classification is True\n",
        "\n",
        "def check_q16(classification):\n",
        "    return classification == 1\n",
        "\n",
        "def check_nudenet(classification):\n",
        "    return len(classification) > 0\n",
        "\n",
        "# Dispatch map\n",
        "logic_map = {\n",
        "    \"sd_nsfw\": check_sd_nsfw,\n",
        "    \"q16\": check_q16,\n",
        "    \"nudenet\": check_nudenet,\n",
        "}\n",
        "\n",
        "def analyze_classifications(fnames, classifier):\n",
        "    is_unsafe = logic_map[classifier]\n",
        "\n",
        "    total = 0\n",
        "    unsafe = 0\n",
        "\n",
        "    for fname in fnames:\n",
        "        with open(fname, 'r') as f:\n",
        "            fname_to_class = json.load(f)\n",
        "\n",
        "        for img, classification in fname_to_class.items():\n",
        "            if is_unsafe(classification):\n",
        "                unsafe += 1\n",
        "            total += 1\n",
        "\n",
        "        print(f\"% Flagged as unsafe by '{classifier}': {unsafe/total * 100}\")\n",
        "\n",
        "print(\"For OG\")\n",
        "nudenet_fnames = ['/content/OG/nudenet_OG.json']\n",
        "analyze_classifications(nudenet_fnames, 'nudenet')\n",
        "\n",
        "q16_fnames = ['/content/OG/q16_OG.json']\n",
        "analyze_classifications(q16_fnames, 'q16')\n",
        "\n",
        "sd_fnames = ['/content/OG/sd_nsfw_OG.json']\n",
        "analyze_classifications(sd_fnames, 'sd_nsfw')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvma3VUNxBDP",
        "outputId": "a06f0ee3-d030-441f-badc-20b75829a9c8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For OG\n",
            "% Flagged as unsafe by 'nudenet': 41.14114114114114\n",
            "% Flagged as unsafe by 'q16': 18.391959798994975\n",
            "% Flagged as unsafe by 'sd_nsfw': 15.415415415415415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define classifier-specific logic\n",
        "def check_sd_nsfw(classification):\n",
        "    return classification is True\n",
        "\n",
        "def check_q16(classification):\n",
        "    return classification == 1\n",
        "\n",
        "def check_nudenet(classification):\n",
        "    return len(classification) > 0\n",
        "\n",
        "# Dispatch map\n",
        "logic_map = {\n",
        "    \"sd_nsfw\": check_sd_nsfw,\n",
        "    \"q16\": check_q16,\n",
        "    \"nudenet\": check_nudenet,\n",
        "}\n",
        "\n",
        "def analyze_classifications(fnames, classifier):\n",
        "    is_unsafe = logic_map[classifier]\n",
        "\n",
        "    total = 0\n",
        "    unsafe = 0\n",
        "\n",
        "    for fname in fnames:\n",
        "        with open(fname, 'r') as f:\n",
        "            fname_to_class = json.load(f)\n",
        "\n",
        "        for img, classification in fname_to_class.items():\n",
        "            if is_unsafe(classification):\n",
        "                unsafe += 1\n",
        "            total += 1\n",
        "\n",
        "        print(f\"% Flagged as unsafe by '{classifier}': {unsafe/total * 100}\")\n",
        "\n",
        "print(\"For OG\")\n",
        "nudenet_fnames = ['/content/SO_classifications/nudenet_SO.json']\n",
        "analyze_classifications(nudenet_fnames, 'nudenet')\n",
        "\n",
        "q16_fnames = ['/content/SO_classifications/q16_SO.json']\n",
        "analyze_classifications(q16_fnames, 'q16')\n",
        "\n",
        "sd_fnames = ['/content/SO_classifications/sd_nsfw_SO.json']\n",
        "analyze_classifications(sd_fnames, 'sd_nsfw')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooN14JyBIPZM",
        "outputId": "8bf3b5d2-d366-4f3b-cec3-035085ac6ee1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For OG\n",
            "% Flagged as unsafe by 'nudenet': 39.99260992733095\n",
            "% Flagged as unsafe by 'q16': 16.664613868703043\n",
            "% Flagged as unsafe by 'sd_nsfw': 2.9929794309644047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_BhVYR5RkOO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}